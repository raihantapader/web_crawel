web-crawler/
├── src/crawler/           ← 12 core modules
│   ├── __init__.py
│   ├── models.py          # CrawlRequest, CrawlResult, CrawlStats
│   ├── config.py          # CrawlerConfig
│   ├── crawler.py         # WebCrawler orchestrator
│   ├── worker.py          # Async CrawlWorker
│   ├── fetcher.py         # Static + Dynamic fetchers
│   ├── parser.py          # Content extraction
│   ├── link_extractor.py  # URL discovery
│   ├── frontier.py        # Memory + Redis queues
│   ├── storage.py         # JSON + MongoDB storage
│   ├── rate_limiter.py    # Per-domain throttling
│   └── robots_handler.py  # robots.txt
├── tests/                 ← 7 test files (>70% coverage)
├── examples/              ← 3 working scripts
│   ├── example_static_crawl.py
│   ├── example_dynamic_crawl.py
│   └── example_custom_extractors.py
├── Dockerfile
├── docker-compose.yml
├── requirements.txt
└── README.md